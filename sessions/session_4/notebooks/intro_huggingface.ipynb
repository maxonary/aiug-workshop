{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (4.43.3)\n",
      "Requirement already satisfied: torch in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/maximilianarnold/Repositories/aiug-workshop/venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model. For example, if you think about it, languages all run on an underlying machine, something that makes sense,\"},\n",
       " {'generated_text': \"Hello, I'm a language modeler. You guys always think that I'm a complete wannabe programmer. I am. I understand that it\"},\n",
       " {'generated_text': \"Hello, I'm a language modeler. And I don't want that to become all a language model out of this very-slow machine learning.\"},\n",
       " {'generated_text': \"Hello, I'm a language model programmer, so I am not going to go down too many rabbit holes from there. I am happy right now,\"},\n",
       " {'generated_text': \"Hello, I'm a language modeler. I take an idea and figure out what the hell it could do and why. What's it doing to\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(\"text-generation\")\n",
    "\n",
    "model(\"Hello, I'm a language model\", \n",
    "      max_length=30, \n",
    "      num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name =\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "save_directory = \"model_directory\"\n",
    "\n",
    "# load from pretrained instances\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# save model for later user\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding:\n",
      "{'input_ids': [1, 15043, 29892, 306, 29915, 29885, 263, 4086, 1904], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Decoding:\n",
      "<s> Hello, I'm a language model\n"
     ]
    }
   ],
   "source": [
    "# tokenizer outputs\n",
    "encoding = tokenizer(\"Hello, I'm a language model\")\n",
    "\n",
    "# encoding\n",
    "print(\"Encoding:\")\n",
    "print(encoding)\n",
    "\n",
    "# decoding\n",
    "print(\"Decoding:\")\n",
    "print(tokenizer.decode(encoding['input_ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model. How can I use it to generate text?\\n\\nLanguage model: Sure, I can generate text based on your prompt. Here's an example:\\n\\nPrompt: Write a heartfelt letter to your favorite musician, expressing your gratitude for their music and how it has impacted your life.\\n\\nGenerated text: Dear [Musician],\\n\\nI hope this letter finds you well. I've been listening to your music for years, and it's been a constant source of inspiration for me. Your music has touched my heart in ways that words can't express, and I want to express my gratitude for that.\\n\\nYour music has brought me joy, comfort, and a sense of belonging. It's been a constant reminder that music can be a powerful tool for connecting with others and expressing emotions. Your music has helped me through some of the toughest times in my life, and I'm grateful for that.\\n\\nYour music has also had a profound impact on my life. It's helped me to see the world in a different way, to appreciate the beauty in everyday things, and to find solace in the moments that matter most.\\n\\nI'm so grateful for your music, and I hope you know how much it means to me. Thank you for all that you do.\\n\\nSincerely,\\n\\n[Your Name]\"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model from saved directory\n",
    "model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "model = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0) # device=0 for GPU, device=-1 for CPU\n",
    "\n",
    "model(\"Hello, I'm a language model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
